{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Import necessary libraries\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, Dense, Dropout, concatenate\nfrom tensorflow.keras.models import Model\nfrom transformers import TFAutoModel, AutoTokenizer\n\n# Load the train and test datasets\ntrain_df = pd.read_csv('/kaggle/input/watson/train.csv')\ntest_df = pd.read_csv('/kaggle/input/watson/test.csv')\n\n# Define the mapping of language abbreviations to full names\nlanguage_mapping = {'ar':'Arabic', 'bg':'Bulgarian', 'de':'German', 'el':'Greek', 'en':'English', 'es':'Spanish', 'fr':'French', 'hi':'Hindi', 'ru':'Russian', 'sw':'Swahili', 'th':'Thai', 'tr':'Turkish', 'ur':'Urdu', 'vi':'Vietnamese', 'zh':'Chinese'}\n\n# Convert language abbreviations to full names in the train and test datasets\ntrain_df['language'] = train_df['language'].apply(lambda x: language_mapping.get(x, x))\ntest_df['language'] = test_df['language'].apply(lambda x: language_mapping.get(x, x))\n\n# Define the maximum sequence length for the inputs\nMAX_LEN = 128\n\n# Load the pre-trained BERT tokenizer\ntokenizer = AutoTokenizer.from_pretrained('bert-base-multilingual-cased')\n\n# Tokenize the train and test data using the BERT tokenizer\ntrain_encoded = tokenizer(list(train_df['premise']), list(train_df['hypothesis']), padding='max_length', truncation=True, max_length=MAX_LEN, return_tensors='tf')\ntest_encoded = tokenizer(list(test_df['premise']), list(test_df['hypothesis']), padding='max_length', truncation=True, max_length=MAX_LEN, return_tensors='tf')\n\n# Convert the labels to numpy arrays\ntrain_labels = np.array(train_df['label'])\n\n# Split the train dataset into a training set and a validation set\nVALID_SPLIT = 0.2\nidx = int(len(train_encoded['input_ids']) * (1 - VALID_SPLIT))\ntrain_inputs = {k: v[:idx] for k, v in train_encoded.items()}\nval_inputs = {k: v[idx:] for k, v in train_encoded.items()}\ntrain_labels = train_labels[:idx]\nval_labels = train_labels[idx:]\n\n# Define the BERT-based NLI model\ndef create_model():\n    bert_model = TFAutoModel.from_pretrained('bert-base-multilingual-cased')\n    input_ids = Input(shape=(MAX_LEN,), dtype=tf.int32, name='input_ids')\n    attention_mask = Input(shape=(MAX_LEN,), dtype=tf.int32, name='attention_mask')\n    token_type_ids = Input(shape=(MAX_LEN,), dtype=tf.int32, name='token_type_ids')\n    bert_output = bert_model({'input_ids': input_ids, 'attention_mask': attention_mask, 'token_type_ids': token_type_ids})[1]\n    bert_output = Dropout(0.2)(bert_output)\n    dense1 = Dense(128, activation='relu')(bert_output)\n    dense1 = Dropout(0.2)(dense1)\n    dense2 = Dense(64, activation='relu')(dense1)\n    output = Dense(3, activation='softmax')(dense2)\n    model = Model(inputs=[input_ids, attention_mask, token_type_ids], outputs=output)\n    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model\n\n# Create the BERT-based NLI model\nmodel = create_model()\n\n# Train the model on the train dataset\nBATCH_SIZE = 32\nEPOCHS = 3\nhistory = model.fit(train_inputs, train_labels, batch_size=BATCH_SIZE, epochs=EPOCHS, validation_data=(val_inputs, val_labels))\n\n# Evaluate the model on the validation set\nmodel.evaluate(val_inputs, val_labels)\n\n# Make predictions on the test set\ntest_predictions = np.argmax(model.predict(test_encoded), axis=-1)\n\n# Save the predictions to a CSV file in the required format\nsubmission_df = pd.DataFrame({'id': test_df['id'], 'prediction': test_predictions})\nsubmission_df.to_csv('submission.csv', index=False)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-03-09T09:28:02.842325Z","iopub.execute_input":"2023-03-09T09:28:02.842834Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"add1176218b54253986d552770fd33b4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dcfd45da8c5f4ced900df9c5b6ae8143"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf3418894eaf4d2480075f6c6286253a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a44602653fe42a9848e0be1935998ef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)\"tf_model.h5\";:   0%|          | 0.00/1.08G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"12e1519d91f74cba8ba703f4cdef271c"}},"metadata":{}},{"name":"stderr","text":"Some layers from the model checkpoint at bert-base-multilingual-cased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAll the layers of TFBertModel were initialized from the model checkpoint at bert-base-multilingual-cased.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3\n 11/303 [>.............................] - ETA: 1:48:21 - loss: 1.3434 - accuracy: 0.3210","output_type":"stream"}]}]}